{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c0dd9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb557cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1xRU3xY5-tkiPGvlz5xBJ18_pHWSRzI4v\n",
      "To: D:\\Git\\Courses-and-Certificates\\1 - Coursera\\DeepLearning.AI TensorFlow Developer\\Course 3\\Week 2\\sarcasm.json\n",
      "\n",
      "  0%|          | 0.00/5.64M [00:00<?, ?B/s]\n",
      "  9%|9         | 524k/5.64M [00:00<00:05, 903kB/s]\n",
      " 19%|#8        | 1.05M/5.64M [00:00<00:04, 1.11MB/s]\n",
      " 28%|##7       | 1.57M/5.64M [00:01<00:02, 1.49MB/s]\n",
      " 37%|###7      | 2.10M/5.64M [00:01<00:01, 1.78MB/s]\n",
      " 46%|####6     | 2.62M/5.64M [00:01<00:01, 1.94MB/s]\n",
      " 56%|#####5    | 3.15M/5.64M [00:01<00:01, 2.10MB/s]\n",
      " 65%|######5   | 3.67M/5.64M [00:02<00:00, 2.11MB/s]\n",
      " 74%|#######4  | 4.19M/5.64M [00:02<00:00, 2.20MB/s]\n",
      " 84%|########3 | 4.72M/5.64M [00:02<00:00, 2.35MB/s]\n",
      " 93%|#########2| 5.24M/5.64M [00:02<00:00, 2.33MB/s]\n",
      "100%|##########| 5.64M/5.64M [00:02<00:00, 2.33MB/s]\n",
      "100%|##########| 5.64M/5.64M [00:02<00:00, 1.96MB/s]\n"
     ]
    }
   ],
   "source": [
    "#!pip install gdown\n",
    "!gdown --id 1xRU3xY5-tkiPGvlz5xBJ18_pHWSRzI4v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "258d6fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sarcasm.json','r') as f:\n",
    "    datastore = json.load(f)\n",
    "\n",
    "sentences=[]\n",
    "labels=[]\n",
    "urls= []\n",
    "for row in datastore:\n",
    "    sentences.append(row['headline'])\n",
    "    labels.append(row[\"is_sarcastic\"])\n",
    "    urls.append(row['article_link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c86621c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the dataset:  26709\n",
      "Training size:  20000\n"
     ]
    }
   ],
   "source": [
    "print(\"Total size of the dataset: \",len(sentences))\n",
    "training_size=20000\n",
    "print(\"Training size: \", training_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4df7545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences = sentences[:training_size]\n",
    "testing_sentences = sentences[training_size:]\n",
    "training_labels = labels[:training_size]\n",
    "testing_labels = labels[training_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb61ce7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_size = 10000\n",
    "maxlen = 120\n",
    "embending_dim =16\n",
    "oov_token = \"<OOV>\"\n",
    "padding_type = \"post\"\n",
    "truncating_type = \"post\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65eeb8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=corpus_size, oov_token=oov_token)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "training_padded = pad_sequences(training_sequences,\n",
    "                             maxlen=maxlen,\n",
    "                             padding=padding_type,\n",
    "                             truncating=truncating_type)\n",
    "testing_padded = pad_sequences(testing_sequences,\n",
    "                            maxlen=maxlen,\n",
    "                            padding=padding_type,\n",
    "                            truncating=truncating_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e69f95e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_padded = np.array(training_padded)\n",
    "testing_padded = np.array(testing_padded)\n",
    "\n",
    "training_labels = np.array(training_labels)\n",
    "testing_labels = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b79c0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 120, 16)           160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 160,577\n",
      "Trainable params: 160,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=corpus_size, output_dim=embending_dim, input_length=maxlen),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(32,activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam', loss=\"binary_crossentropy\", metrics=\"acc\"\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e72faf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.6447 - acc: 0.6152 - val_loss: 0.4996 - val_acc: 0.7995\n",
      "Epoch 2/30\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.3869 - acc: 0.8399 - val_loss: 0.3730 - val_acc: 0.8407\n",
      "Epoch 3/30\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2962 - acc: 0.8801 - val_loss: 0.3574 - val_acc: 0.8436\n",
      "Epoch 4/30\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2494 - acc: 0.9014 - val_loss: 0.3530 - val_acc: 0.8436\n",
      "Epoch 5/30\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.2161 - acc: 0.9150 - val_loss: 0.3529 - val_acc: 0.8496\n",
      "Epoch 6/30\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1895 - acc: 0.9275 - val_loss: 0.3610 - val_acc: 0.8536\n",
      "Epoch 7/30\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1713 - acc: 0.9336 - val_loss: 0.3785 - val_acc: 0.8527\n",
      "Epoch 8/30\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1522 - acc: 0.9442 - val_loss: 0.3883 - val_acc: 0.8535\n",
      "Epoch 9/30\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1354 - acc: 0.9502 - val_loss: 0.4088 - val_acc: 0.8501\n",
      "Epoch 10/30\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1247 - acc: 0.9544 - val_loss: 0.4326 - val_acc: 0.8483\n",
      "Epoch 11/30\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1139 - acc: 0.9593 - val_loss: 0.4553 - val_acc: 0.8472\n",
      "Epoch 12/30\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.1036 - acc: 0.9630 - val_loss: 0.4967 - val_acc: 0.8371\n",
      "Epoch 13/30\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0948 - acc: 0.9662 - val_loss: 0.5139 - val_acc: 0.8414\n",
      "Epoch 14/30\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0860 - acc: 0.9691 - val_loss: 0.5682 - val_acc: 0.8325\n",
      "Epoch 15/30\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0789 - acc: 0.9739 - val_loss: 0.5803 - val_acc: 0.8356\n",
      "Epoch 16/30\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0732 - acc: 0.9754 - val_loss: 0.6777 - val_acc: 0.8217\n",
      "Epoch 17/30\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0683 - acc: 0.9768 - val_loss: 0.6492 - val_acc: 0.8320\n",
      "Epoch 18/30\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0617 - acc: 0.9800 - val_loss: 0.6776 - val_acc: 0.8307\n",
      "Epoch 19/30\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0564 - acc: 0.9817 - val_loss: 0.7345 - val_acc: 0.8259\n",
      "Epoch 20/30\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0546 - acc: 0.9819 - val_loss: 0.7514 - val_acc: 0.8271\n",
      "Epoch 21/30\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0474 - acc: 0.9853 - val_loss: 0.7980 - val_acc: 0.8235\n",
      "Epoch 22/30\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0453 - acc: 0.9857 - val_loss: 0.8387 - val_acc: 0.8223\n",
      "Epoch 23/30\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0412 - acc: 0.9872 - val_loss: 0.8765 - val_acc: 0.8229\n",
      "Epoch 24/30\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0378 - acc: 0.9882 - val_loss: 0.9311 - val_acc: 0.8182\n",
      "Epoch 25/30\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0364 - acc: 0.9877 - val_loss: 0.9555 - val_acc: 0.8192\n",
      "Epoch 26/30\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0323 - acc: 0.9901 - val_loss: 1.0479 - val_acc: 0.8129\n",
      "Epoch 27/30\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0324 - acc: 0.9896 - val_loss: 1.0426 - val_acc: 0.8155\n",
      "Epoch 28/30\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0287 - acc: 0.9910 - val_loss: 1.1052 - val_acc: 0.8120\n",
      "Epoch 29/30\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0252 - acc: 0.9924 - val_loss: 1.0974 - val_acc: 0.8134\n",
      "Epoch 30/30\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.0250 - acc: 0.9918 - val_loss: 1.1524 - val_acc: 0.8131\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "history = model.fit(\n",
    "    training_padded,\n",
    "    training_labels,\n",
    "    batch_size=20,\n",
    "    epochs=num_epochs,\n",
    "    verbose=1,\n",
    "    validation_data=(testing_padded,testing_labels)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70a4ce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index = dict([(value,key) for (key, value) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fe699561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sentence(text):\n",
    "    return ' '.join([reverse_word_index.get(i,'?') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32a87743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "former <OOV> store clerk sues over secret 'black <OOV> for minority shoppers ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?\n",
      "mom starting to fear son's web series closest thing she will have to grandchild\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(decode_sentence(training_padded[0]))\n",
    "print(training_sentences[2])\n",
    "print(labels[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a52ff67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "e = model.layers[0]\n",
    "weights =e.get_weights()[0]\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fa71ce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "for word_num in range(1, corpus_size):\n",
    "  word = reverse_word_index[word_num]\n",
    "  embeddings = weights[word_num]\n",
    "  out_m.write(word + \"\\n\")\n",
    "  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "64d5e73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.095522e-01]\n",
      " [3.983242e-06]]\n"
     ]
    }
   ],
   "source": [
    "# for collab\n",
    "\"\"\"try:\n",
    "  from google.colab import files\n",
    "except ImportError:\n",
    "  pass\n",
    "else:\n",
    "  files.download('vecs.tsv')\n",
    "  files.download('meta.tsv')\n",
    "  \"\"\"\n",
    "sentence = [\"granny starting to fear spiders in the garden might be real\", \"game of thrones season finale showing this sunday night\"]\n",
    "sequences = tokenizer.texts_to_sequences(sentence)\n",
    "padded = pad_sequences(sequences, maxlen=maxlen, padding=padding_type, truncating=truncating_type)\n",
    "print(model.predict(padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55a74ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
